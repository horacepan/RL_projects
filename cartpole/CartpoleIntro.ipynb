{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Processes\n",
    "A [Markov Decision Process](https://en.wikipedia.org/wiki/Markov_decision_process) is formally a 5-tuple $(S, A, P, R, \\gamma)$. In the following, we denote $s_t$ as the state at time $t$, $a_t$ the action taken at time $t$.\n",
    "* $S$: the state space\n",
    "\n",
    "* $A$: the action space, or set of actions you are allowed to take\n",
    "\n",
    "* $P$: a probability distribution over $S \\times S \\times A$\n",
    "$$P(s, s', a) = Prob(s_{t+1} = s' | s_t = s, a_t = a)$$\n",
    "\n",
    "* $R$: the reward function. Formally you can think of $R$ as a function from $S \\times A$ to the reals.\n",
    "$$R(s_t, a_t) = \\text{the reward you get from taking action $a_t$ at state $s_t$}$$\n",
    "\n",
    "* $\\gamma$: the discount factor which tells you how much to weigh future rewards versus present rewards. For now, we'll ignore this.\n",
    "\n",
    "\n",
    "Whenever you have a problem that can be formalized as an MDP you are generally interested in finding a **policy** that will maximize your long term reward. We will formalize this later on, but here are some examples of things that can be formalized as MDPS:\n",
    "* chess: The reward in these can be modeled as 0 if the game has not yet terminated, -1 if your action leads to you losing the game in your opponents next move, +1 if your action leads to a win in the next move. Note that you can also use a different reward function here, where your reward is equal to the point value of the piece you just captured (0 if no capture), but this can lead to myopic policies that are good at taking pieces while losing you the game.\n",
    "\n",
    "* breakout: Here the state space is the configuration of bricks present. The action space is just [move paddle left, move paddle right]. The reward function might be the number of bricks broken or the number of game frames that the ball spends aloft.\n",
    "\n",
    "\n",
    "From these two basic examples of MDP's, we can see that we'll often have a predetermined state /action space and a fixed transition probability function(well in chess, the transition depends on your opponent). The reward function, however, is something that you can play around with. In the next section we'll see an example the Cartpole problem which is a classic control task that can be modeled as an MDP and thus solved with **reinforcement learning** techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Cart Pole Environment\n",
    "In the Cartpole problem we have a pole that is attached to a cart. The goal is to keep the pole balanced for as long as possible. Given some initial unbalanced configuration of pole on the cart, you'll have to move the cart to the left or to the right to keep the pole from falling over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "rgb = env.render(mode='rgb_array')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11bf81c18>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cartpole State/Action Space/Reward\n",
    "The cartpole states are parameterized by 4 real numbers. I think two of them correspond to angles and the other two correspond to something like distance from origin? But I'm not entirely sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State(aka observation) space: Box(4,)\n",
      "Action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print(\"State(aka observation) space: {}\".format(env.observation_space))\n",
    "print(\"Action space: {}\".format(env.action_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box is a class defined in gym.spaces.box. It is generally used  to represents a vector in $R_n$. In the Cartpole environment, the observation(or state) space is just $R^4$.\n",
    "\n",
    "The action space is just $\\{0, 1 \\}$. This corresponds to moving the cart left or right.\n",
    "\n",
    "The reward is +1 if the pole is still up after taking the given action and 0 if the pole falls over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Winning\" the Cartpole Environment\n",
    "We say that the cartpole problem is \"solved\" when we have found a policy that can keep the pole up for more than 195 timesteps. This is admittedly fairly arbitrary, but for our purposes, it's good enough.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Linear Decision Model\n",
    "This problem of keeping the pole up can be reduced to a pseudo-supervised classification problem. The most naive thing we can try to do is to make a linear decision function of the flavor:\n",
    "$$\n",
    "f_{w, b}(x)= \n",
    "\\begin{cases}\n",
    "    0,& w^T s_t + b \\geq 0\\\\\n",
    "    1,              & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "where $x \\in \\mathbb{R}^4$ is your state vector , $w \\in \\mathbb{R}^4, b \\in \\mathbb{R}$. The problem with this is that this doesn't quite fit in the classic supervised learning framework. There is no \"ground truth\" label to predict here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy\n",
    "We can still get a notion of \"good\" values for $w$ and $b$ based on how good this decision function $f_{w, b}$ is in practice. So use $f_{w, b}$ to determine which action to take at each time step and then look the total reward that this policy produces(ie: how long does a pole stay upright if we use this function to determine whether to move left or right at each time step). This notion of \"quality\" can then guide us in what $w$ and $b$ to use.\n",
    "\n",
    "The [Cross Entropy Method](https://en.wikipedia.org/wiki/Cross-entropy_method) broadly be seen as a sort of evolutionary method for optimization. We want to find a weight vector $w \\in \\mathbb{R}^4$, bias term $b \\in \\mathbb{R}$ so that the decision function $f_{w, b}$ can lead to high long term rewards. Broadly speaking, the CEM consists of repeating the following three steps:\n",
    "a) generating a random sample of candidate variables(in our case it will be the $w$ and $b$s) according to some distribution\n",
    "b) evaluating each sample's \"fitness\"(total reward)\n",
    "c) keeping the top K candidate variables with highest \"fitness\" and update the sampling procedure with these top K values to get a better performing sample in the next iteration.\n",
    "\n",
    "We'll model $w$ and $b$ as random variables drawn from a normal distribution with unknown mean $\\mu$ and variance $\\sigma^2$. Note that we can think of the $[w, b]$ as a vector in $\\mathbb{R}^5$. So to save on typing, I'll refer to\n",
    "the combination of $w$ and $b$ being drawn from a 5-dimensional multivariate normal distribution.\n",
    "\n",
    "The process for learning these $w$ and $b$ values is as follows:\n",
    "1. Start with $\\mu = [0,0,0, 0, 0]$, $\\sigma=1$. This initialization is a bit arbitrary. You can use a different initialization if you'd like.\n",
    "2. generate random batch of say 50 $w$ and $b$ values according to $\\mathcal{N}(\\mu, \\sigma^2)$\n",
    "3. For each sample of $w$ and $b$, run the policy that function $f_{w, b}$ induces. Keep track of the reward each function produced.\n",
    "4. Keep the top ten or so $w, b$ values (as defined by the total reward that their corresponding $f_{w, b}$ functions attained)\n",
    "5. Update $\\mu$ and $\\sigma$ with the mean and standard dev of the top 10 $w$ and $b$ values found in the previous step\n",
    "6. Go back to step 2.\n",
    "\n",
    "This process is repeated until the $w$ and $b$ values that are sampled produce a good enough policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_action(observation, weights):\n",
    "    '''\n",
    "    observation: length 4 array\n",
    "    weights: length 5 array(bias term is included here)\n",
    "    This computes the f_{w, b} decision function described above\n",
    "    '''\n",
    "    w = weights[:-1]\n",
    "    b = weights[-1]\n",
    "    if w.dot(observation) + b > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(env, weight, max_steps):\n",
    "    '''\n",
    "    This function uses the given weight(aka w and b in the writeup above)\n",
    "    as the policy for determining actions\n",
    "    env: gym environment\n",
    "    weight: 5-dim numpy array\n",
    "    max_steps: maximum number of time steps to run the environment for\n",
    "    '''\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    for i in range(max_steps):\n",
    "        action = get_action(obs, weight)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    num_steps = i\n",
    "    \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 (episodes 0-50) | Batch avg: 24.40 | Running avg 24.40\n",
      "Batch 1 (episodes 50-100) | Batch avg: 49.54 | Running avg 36.97\n",
      "Batch 2 (episodes 100-150) | Batch avg: 68.62 | Running avg 47.52\n",
      "Batch 3 (episodes 150-200) | Batch avg: 90.30 | Running avg 58.22\n",
      "Batch 4 (episodes 200-250) | Batch avg: 120.64 | Running avg 70.70\n",
      "Batch 5 (episodes 250-300) | Batch avg: 143.74 | Running avg 82.87\n",
      "Batch 6 (episodes 300-350) | Batch avg: 176.88 | Running avg 96.30\n",
      "Batch 7 (episodes 350-400) | Batch avg: 170.56 | Running avg 105.58\n",
      "Batch 8 (episodes 400-450) | Batch avg: 181.04 | Running avg 113.97\n",
      "Batch 9 (episodes 450-500) | Batch avg: 189.72 | Running avg 121.54\n",
      "Batch 10 (episodes 500-550) | Batch avg: 188.58 | Running avg 127.64\n",
      "Batch 11 (episodes 550-600) | Batch avg: 198.20 | Running avg 133.52\n",
      "Done! Last 50 steps have an average total reward of 198.20\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "# w will \n",
    "max_rollouts = 100\n",
    "batch_size = 50\n",
    "reward_history = []\n",
    "weight_mean = np.random.normal(0, 1, size=5)\n",
    "weight_std = 1\n",
    "max_steps = 300\n",
    "\n",
    "for e in range(max_rollouts):\n",
    "    # sampling form N(mu, sigma^2) is the same as sampling from mu + sigma * N(0, 1)\n",
    "    w_samples = np.ones((batch_size, 1)) * weight_mean + \\\n",
    "                weight_std * np.random.normal(0, 1, (batch_size, 5))\n",
    "        \n",
    "    # This gives us a list of tuples of (total reward generated, weight vector)\n",
    "    rollout_results = [(run(env, w, max_steps), w) for w in w_samples]\n",
    "    \n",
    "    # sort the results based on the total reward\n",
    "    sorted_results = sorted(rollout_results, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    # unzip the list of tuples\n",
    "    steps, weights = [list(i) for i in zip(*sorted_results)]\n",
    "    \n",
    "    # update the mean/std of the normal distribution\n",
    "    weight_mean = np.mean(weights[:10], axis=0)\n",
    "    weight_std = np.std(weights[:10], axis=0)\n",
    "    reward_history.extend(steps)\n",
    "    \n",
    "    print(\"Batch {} (episodes {}-{}) | Batch avg: {:.2f} | Running avg {:.2f}\".format(e, e * batch_size, (e+1) * batch_size, np.mean(steps), np.mean(reward_history)))\n",
    "    if np.mean(reward_history[-50:]) > 195:\n",
    "        print(\"Done! Last 50 steps have an average total reward of {:.2f}\".format(np.mean(reward_history[-50:])))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3(env)",
   "language": "python",
   "name": "envname"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
